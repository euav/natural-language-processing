{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word CNN (FastText)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "oSRee2EcvTIa",
        "ABfd_fdcvTIj"
      ],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hcEul10vTHX",
        "colab_type": "text"
      },
      "source": [
        "# Dependencies\n",
        "Let's download and import all the needed libraries.\n",
        "- **NumPy** is needed for intermidiate computaions,\n",
        "- **Pandas** is needed to store data,\n",
        "- **Python's RE** is needed for text preprocessing,\n",
        "- **lxml** is needed to parse data files stored in XML format,\n",
        "- **Gensim** is needed to use FastText embedding,\n",
        "- **Scikit-learn** is needed to evaluate F1 score,\n",
        "- **MXNet** is choosed as deep learning framework,\n",
        "- **google.colab.drive** is need to mount Google Drive with all the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m98pnIouvcWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw-DXmVMvTHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import lxml.etree\n",
        "import smart_open\n",
        "from gensim.utils import tokenize\n",
        "from gensim.models.fasttext import FastText\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import mxnet as mx\n",
        "import mxnet.ndarray as nd\n",
        "import mxnet.gluon as gluon\n",
        "import mxnet.autograd as autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC4gKw3wDR5L",
        "colab_type": "code",
        "outputId": "2894f0e0-911f-4008-9d53-0ea05e8b5c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "DRIVE_PATH = '/content/gdrive/My Drive/NLP Data/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "ujUsZYOPvTHl",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading\n",
        "Data should by extracted from XML files: tweets and their sentiments.\n",
        "And then before we can apply any algorithm data should be normalized.\n",
        "\n",
        "**Tweets normalization**\n",
        "- delete URLs\n",
        "- delete user tags\n",
        "- delete non-russian letters\n",
        "\n",
        "**Sentiments normalization**\n",
        "\n",
        "In data provided sentiments is given company wise, so it is need to assign sentiment on corresponding company to the whole tweet. In case of multiple different sentiments the given tweet will be assumed as sum of all sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qur30qQvTHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_tweet(string):\n",
        "    string = re.sub(r'(?:http[^\\s]+)($|\\s)', '', string)\n",
        "    string = re.sub(r'(?:@[^\\s]+)($|\\s)', '', string)\n",
        "    string = re.sub(r'[^абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ0123456789()!?\\- ]', '', string)\n",
        "    return string\n",
        "\n",
        "def normalize_sentiment(raw_sentiments_on_companies):\n",
        "    sentiment = 0\n",
        "    for raw_value in raw_sentiments_on_companies:\n",
        "        if raw_value.text != 'NULL':\n",
        "            sentiment += int(raw_value.text)\n",
        "    return np.sign(sentiment)\n",
        "\n",
        "def load_dataframe(filename, n_companies):\n",
        "    tweets = []\n",
        "    sentiments = []\n",
        "    \n",
        "    for sample in lxml.etree.parse(filename).xpath('database/table'):\n",
        "        tweet = normalize_tweet(sample[3].text)\n",
        "        sentiment = normalize_sentiment(sample[4:4+n_companies])\n",
        "\n",
        "        tweets.append(tweet)\n",
        "        sentiments.append(sentiment)\n",
        "    \n",
        "    return pd.DataFrame({'tweet': tweets, 'sent': sentiments})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_hCvi5zvTHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bank_train = load_dataframe(DRIVE_PATH + 'SentiRuEval/bank_train_2016.xml', n_companies=8)\n",
        "bank_test = load_dataframe(DRIVE_PATH + 'SentiRuEval/bank_test_etalon.xml', n_companies=8)\n",
        "\n",
        "comm_train = load_dataframe(DRIVE_PATH + 'SentiRuEval/tkk_train_2016.xml', n_companies=7)\n",
        "comm_test = load_dataframe(DRIVE_PATH + 'SentiRuEval/tkk_test_etalon.xml', n_companies=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSt-4w1jvTH0",
        "colab_type": "text"
      },
      "source": [
        "Let's see the data we are working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv66swvOFGz1",
        "colab_type": "code",
        "outputId": "8e8d49e0-4355-4fa9-da37-89f9086e98bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "bank_train.head(20)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Взять кредит тюмень альфа банк</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Мнение о кредитной карте втб 24</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Райффайзенбанк Снижение ключевой ставки ЦБ на ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Современное состояние кредитного поведения в р...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Главное чтоб банки СБЕР и ВТБ!!!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Оформить краткосрочный кредит оао банк москвы</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Самый выгодный автокредит в втб 24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Кредит иногородним в москве сбербанк</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Кредитный калькулятор россельхозбанк чита</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Легко можно получить денежный кредит ы втб 24 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ДЕЛО -  На 1 млрд грн ушел в минус Райффайзен ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Део нексия в кредит в альфа банке</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Россельхозбанк кредит стань фермером</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Банк втб взять кредит 40000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>обязательно про сбербанк напишите! Временами п...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Расчитать кредит сбер банк россии в москве</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Кредит в газпром банке тула</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Кредитный калькулятор сбербанк россии</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>втб и сбер точно вопрос-блокировка операций ил...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Взять кредит строительство магазина россельхоз...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                tweet  sent\n",
              "0                      Взять кредит тюмень альфа банк     0\n",
              "1                    Мнение о кредитной карте втб 24      0\n",
              "2   Райффайзенбанк Снижение ключевой ставки ЦБ на ...     0\n",
              "3   Современное состояние кредитного поведения в р...     0\n",
              "4                    Главное чтоб банки СБЕР и ВТБ!!!     1\n",
              "5       Оформить краткосрочный кредит оао банк москвы     0\n",
              "6                 Самый выгодный автокредит в втб 24      1\n",
              "7               Кредит иногородним в москве сбербанк      0\n",
              "8          Кредитный калькулятор россельхозбанк чита      0\n",
              "9   Легко можно получить денежный кредит ы втб 24 ...     1\n",
              "10  ДЕЛО -  На 1 млрд грн ушел в минус Райффайзен ...    -1\n",
              "11                 Део нексия в кредит в альфа банке      0\n",
              "12              Россельхозбанк кредит стань фермером      0\n",
              "13                        Банк втб взять кредит 40000     0\n",
              "14  обязательно про сбербанк напишите! Временами п...    -1\n",
              "15        Расчитать кредит сбер банк россии в москве      0\n",
              "16                       Кредит в газпром банке тула      0\n",
              "17             Кредитный калькулятор сбербанк россии      0\n",
              "18  втб и сбер точно вопрос-блокировка операций ил...    -1\n",
              "19  Взять кредит строительство магазина россельхоз...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wFaIbKivTIZ",
        "colab_type": "text"
      },
      "source": [
        "# Word CNN (pretrained FastText embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSRee2EcvTIa",
        "colab_type": "text"
      },
      "source": [
        "### Training own FastText embedding\n",
        "In order to train FastText on Julia's dataset it is needed to parse SQL script file. In my solution every line starting with `INSERT` treated as mini-batch. So finding text shifts for different tweets and skipping other fields of database makes possible to extract mini-batch of tweets per line of SQL script.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncCZpjU5r-Kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetsIterator(object):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.insert_prefix = 'INSERT INTO `sentiment` VALUES '\n",
        "        \n",
        "    def __iter__(self):\n",
        "        with smart_open.open(self.path, 'r', encoding='utf-8') as sql_script:\n",
        "            for line in sql_script:\n",
        "                if not line.startswith(self.insert_prefix):\n",
        "                    continue\n",
        "\n",
        "                idx = line.find('(')\n",
        "                while idx != -1:\n",
        "                    idx = line.find(',', idx + 1)\n",
        "                    idx = line.find(',', idx + 1)\n",
        "                    idx = line.find(',', idx + 1) + 1\n",
        "                    begin = idx + 1\n",
        "\n",
        "                    idx = line.find(\"'\", idx + 1)\n",
        "                    while line[idx + 1] == \"'\":\n",
        "                        idx = line.find(\"'\", idx + 1)\n",
        "\n",
        "                    end = idx\n",
        "                    idx = line.find('(', idx)\n",
        "                    tweet = normalize_tweet(line[begin:end])\n",
        "                    yield list(tokenize(tweet))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqob_FAHzVHD",
        "colab_type": "text"
      },
      "source": [
        "**WARNING**: Several hours of execution for training FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWZvlbSXiTVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = FastText()\n",
        "embedding.build_vocab(sentences=TweetsIterator(DRIVE_PATH + 'db.sql'))\n",
        "embedding.train(sentences=TweetsIterator(DRIVE_PATH + 'db.sql'), total_examples=embedding.corpus_count, epochs=10)\n",
        "embedding.save(DRIVE_PATH + 'twitter.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8lUNxSGvTIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "d9b6fc73-a449-4275-8ae5-d9706dd3406b"
      },
      "source": [
        "embedding = FastText.load(DRIVE_PATH + 'twitter.model')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YalwcrAvTIf",
        "colab_type": "text"
      },
      "source": [
        "### Word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtgoLNZ-vTIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = embedding.vector_size\n",
        "MAX_WORDS = 40\n",
        "\n",
        "def tweet_to_matrix(tweet):\n",
        "    matrix = nd.zeros((EMBEDDING_SIZE, MAX_WORDS))\n",
        "    for i, word in enumerate(tweet.split()):\n",
        "        if word in embedding.wv.vocab:\n",
        "            matrix[:, i] = embedding.wv[word]\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def one_hot_label(sentiment):\n",
        "    return (np.arange(3) == (sentiment + 1)).astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABfd_fdcvTIj",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess data before word-level CNN\n",
        "Here we can choose dataset for our investigation:\n",
        "1. Tweets about banks,\n",
        "2. Tweets about telecommunication companies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnP6pQAtdYXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframes = [bank_train, bank_test]\n",
        "#dataframes = [comm_train, comm_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKbrZ106F-vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = nd.zeros((len(dataframes[0]), EMBEDDING_SIZE, MAX_WORDS))\n",
        "y_train = nd.zeros((len(dataframes[0]), 3))\n",
        "                        \n",
        "X_test = nd.zeros((len(dataframes[1]), EMBEDDING_SIZE, MAX_WORDS))\n",
        "y_test = nd.zeros((len(dataframes[1]), 3))\n",
        "\n",
        "for i in range(len(bank_train)):\n",
        "    X_train[i] = tweet_to_matrix(dataframes[0].tweet[i])\n",
        "    y_train[i] = one_hot_label(dataframes[0].sent[i])\n",
        "    \n",
        "for i in range(len(bank_test)):\n",
        "    X_test[i] = tweet_to_matrix(dataframes[1].tweet[i])\n",
        "    y_test[i] = one_hot_label(dataframes[1].sent[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a89uwSe5F_es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = gluon.data.ArrayDataset(X_train, y_train)\n",
        "test_dataset = gluon.data.ArrayDataset(X_test, y_test)\n",
        "\n",
        "train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=1024, shuffle=False)\n",
        "test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Zhc7RIvTI4",
        "colab_type": "text"
      },
      "source": [
        "### Network configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nypR8M5JvTI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordCNN(gluon.nn.HybridBlock):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(WordCNN, self).__init__(**kwargs)\n",
        "        with self.name_scope():\n",
        "            self.conv1 = gluon.nn.Conv1D(channels=100, kernel_size=3, activation='relu')\n",
        "            self.conv2 = gluon.nn.Conv1D(channels=100, kernel_size=4, activation='relu')\n",
        "            self.conv3 = gluon.nn.Conv1D(channels=100, kernel_size=5, activation='relu')\n",
        "            \n",
        "            self.pool = gluon.nn.GlobalMaxPool1D()\n",
        "            self.drop = gluon.nn.Dropout(0.5)\n",
        "            self.fc = gluon.nn.Dense(units=3)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        with x.context:\n",
        "            y1 = self.conv1(x)\n",
        "            y2 = self.conv2(x)\n",
        "            y3 = self.conv3(x)\n",
        "            \n",
        "            z1 = self.pool(y1)\n",
        "            z2 = self.pool(y2)\n",
        "            z3 = self.pool(y3)\n",
        "            \n",
        "            u = nd.concat(z1, z2, z3, dim=1)\n",
        "            v = self.drop(u)\n",
        "            w = self.fc(u)\n",
        "            \n",
        "            return w\n",
        "        \n",
        "net = WordCNN()\n",
        "net.hybridize()\n",
        "softmax = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwMYwdScM2By",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irUaH8mHM07J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.initialize(mx.init.Normal(sigma=0.05), force_reinit=True, ctx=mx.gpu())\n",
        "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 1e-3})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiT_JdlnM71L",
        "colab_type": "code",
        "outputId": "52c94056-7b43-470f-a3b2-bf4fc8e5fdce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "net.collect_params().reset_ctx(mx.gpu())\n",
        "for epoch in range(250):\n",
        "    accumulated_loss = 0\n",
        "    for features, label in train_dataloader:\n",
        "        features = features.as_in_context(mx.gpu())\n",
        "        label = label.as_in_context(mx.gpu())\n",
        "        with autograd.record(train_mode=True):\n",
        "            output = net(features)\n",
        "            loss = softmax(output, label)\n",
        "        loss.backward()\n",
        "        accumulated_loss += nd.mean(loss).asscalar()\n",
        "        trainer.step(batch_size=len(label))\n",
        "  \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('epoch', epoch + 1, '-- train loss', accumulated_loss / len(train_dataloader))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 10 -- train loss 0.3011109381914139\n",
            "epoch 20 -- train loss 0.19143805764615535\n",
            "epoch 30 -- train loss 0.06812883708626032\n",
            "epoch 40 -- train loss 0.0445197707042098\n",
            "epoch 50 -- train loss 0.03419495970010757\n",
            "epoch 60 -- train loss 0.02715745447203517\n",
            "epoch 70 -- train loss 0.02228016182780266\n",
            "epoch 80 -- train loss 0.01886808481067419\n",
            "epoch 90 -- train loss 0.016498337732627988\n",
            "epoch 100 -- train loss 0.015237399842590094\n",
            "epoch 110 -- train loss 0.018640868552029132\n",
            "epoch 120 -- train loss 0.07791596073657274\n",
            "epoch 130 -- train loss 0.40588442608714104\n",
            "epoch 140 -- train loss 0.01711213616654277\n",
            "epoch 150 -- train loss 0.012992948153987527\n",
            "epoch 160 -- train loss 0.011234527151100338\n",
            "epoch 170 -- train loss 0.010167771903797983\n",
            "epoch 180 -- train loss 0.009420553455129266\n",
            "epoch 190 -- train loss 0.008859317540191114\n",
            "epoch 200 -- train loss 0.00842060890281573\n",
            "epoch 210 -- train loss 0.008142104919534177\n",
            "epoch 220 -- train loss 0.008422175701707602\n",
            "epoch 230 -- train loss 0.008153416833374649\n",
            "epoch 240 -- train loss 0.009074800775852054\n",
            "epoch 250 -- train loss 0.007543179031927139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FLSvVZNJ_z",
        "colab_type": "text"
      },
      "source": [
        "### Testing\n",
        "For testing we will use only positive and negative tweets ignoring neutral ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl6qqcbfNMr8",
        "colab_type": "code",
        "outputId": "a2a13d81-fb3c-415a-ee17-027f41533722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "net.collect_params().reset_ctx(mx.cpu())\n",
        "y_true = nd.argmax(y_test, axis=1).asnumpy()\n",
        "y_pred = nd.argmax(net(X_test), axis=1).asnumpy()\n",
        "\n",
        "mask = np.logical_or(y_true == 0, y_true == 2)\n",
        "\n",
        "accuracy = np.mean(y_true[mask] == y_pred[mask])\n",
        "f1_macro = f1_score(y_true[mask], y_pred[mask], average='macro', labels=(0,2))\n",
        "f1_micro = f1_score(y_true[mask], y_pred[mask], average='micro', labels=(0,2))\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('F1-macro:', f1_macro)\n",
        "print('F1-micro:', f1_micro)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.528584817244611\n",
            "F1-macro: 0.5882524894549593\n",
            "F1-micro: 0.6535341830822712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D_8rEO1ebRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}